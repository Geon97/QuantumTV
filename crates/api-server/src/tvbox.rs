use axum::extract::{Json, Query, State};
use axum::http::StatusCode;
use axum::response::IntoResponse;
use base64::Engine;
use quantumtv_core::playback::filter_ads_from_m3_u8;
use serde::{Deserialize, Serialize};
use std::collections::HashSet;
use std::path::PathBuf;
use std::sync::LazyLock;
use std::time::{Duration, SystemTime};

use crate::{AppState, SERVER_IP};
static PARSES_URL: LazyLock<String> = LazyLock::new(|| {
    std::env::var("PARSES_URL").unwrap_or_else(|_| "http://127.0.0.1".to_string())
});
// ================== æ•°æ®ç»“æ„ ==================

#[derive(Clone, Debug)]
pub struct SpiderInfo {
    #[allow(dead_code)]
    pub buffer: Option<Vec<u8>>,
    pub md5: String,
    pub source: String,
    pub success: bool,
    #[allow(dead_code)]
    pub cached: bool,
    pub timestamp: SystemTime,
    #[allow(dead_code)]
    pub size: usize,
    #[allow(dead_code)]
    pub tried: usize,
}

#[derive(Clone, Debug)]
pub struct FailedSources {
    pub sources: HashSet<String>,
    pub last_reset: SystemTime,
}

#[derive(Serialize, Deserialize, Clone, Debug)]
pub struct Site {
    pub key: String,
    pub name: String,
    #[serde(rename = "type")]
    pub site_type: i32,
    pub api: String,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub jar: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub is_adult: Option<bool>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub searchable: Option<i32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    #[serde(rename = "quickSearch")]
    pub quick_search: Option<i32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub filterable: Option<i32>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub changeable: Option<i32>,
}

#[derive(Serialize, Deserialize, Clone, Debug)]
pub struct Parse {
    pub name: String,
    #[serde(rename = "type")]
    pub parse_type: i32,
    pub url: String,
}

#[derive(Serialize, Deserialize, Clone, Debug)]
pub struct SubscriptionConfig {
    #[serde(skip_serializing_if = "Option::is_none")]
    pub spider: Option<String>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub sites: Option<Vec<Site>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub parses: Option<Vec<Parse>>,
    #[serde(skip_serializing_if = "Option::is_none")]
    pub lives: Option<Vec<serde_json::Value>>,
}

#[derive(Deserialize, Clone, Debug)]
pub struct ApiSiteInfo {
    pub name: String,
    pub api: String,
}

#[derive(Deserialize, Clone, Debug)]
pub struct CustomSubscriptionFormat {
    pub api_site: std::collections::HashMap<String, ApiSiteInfo>,
}

#[derive(Deserialize)]
pub struct ConfigParams {
    filter: Option<String>,
    adult: Option<String>,
    mode: Option<String>,
    spider: Option<String>,
    #[serde(rename = "forceSpiderRefresh")]
    force_spider_refresh: Option<String>,
    #[serde(rename = "subscriptionUrl")]
    subscription_url: Option<String>,
}

#[derive(Clone, Debug)]
pub struct CachedSubscription {
    pub config: SubscriptionConfig,
    pub cached_at: SystemTime,
}

/// Spider JAR ç£ç›˜ç¼“å­˜å…ƒæ•°æ®
#[derive(Serialize, Deserialize, Clone, Debug)]
struct SpiderMetadata {
    md5: String,
    source: String,
    success: bool,
    timestamp: u64, // Unix timestamp in seconds
    size: usize,
}

// ================== Spider Jar å€™é€‰æºé…ç½® ==================

const DOMESTIC_CANDIDATES: &[&str] = &[
    "https://agit.ai/Yoursmile7/TVBox/raw/branch/master/jar/custom_spider.jar",
    "https://ghproxy.net/https://raw.githubusercontent.com/FongMi/CatVodSpider/main/jar/custom_spider.jar",
    "https://mirror.ghproxy.com/https://raw.githubusercontent.com/FongMi/CatVodSpider/main/jar/custom_spider.jar",
];

const INTERNATIONAL_CANDIDATES: &[&str] = &[
    "https://raw.githubusercontent.com/FongMi/CatVodSpider/main/jar/custom_spider.jar",
    "https://raw.gitmirror.com/FongMi/CatVodSpider/main/jar/custom_spider.jar",
    "https://ghproxy.cc/https://raw.githubusercontent.com/FongMi/CatVodSpider/main/jar/custom_spider.jar",
];

const PROXY_CANDIDATES: &[&str] = &[
    "https://gh-proxy.com/https://raw.githubusercontent.com/FongMi/CatVodSpider/main/jar/custom_spider.jar",
    "https://ghps.cc/https://raw.githubusercontent.com/FongMi/CatVodSpider/main/jar/custom_spider.jar",
    "https://gh.api.99988866.xyz/https://raw.githubusercontent.com/FongMi/CatVodSpider/main/jar/custom_spider.jar",
];

// Fallback JAR (base64 encoded minimal working spider.jar)
const FALLBACK_JAR_BASE64: &str = "UEsDBBQACAgIACVFfFcAAAAAAAAAAAAAAAAJAAAATUVUQS1JTkYvUEsHCAAAAAACAAAAAAAAACVFfFcAAAAAAAAAAAAAAAANAAAATUVUQS1JTkYvTUFOSUZFU1QuTUZNYW5pZmVzdC1WZXJzaW9uOiAxLjAKQ3JlYXRlZC1CeTogMS44LjBfNDIxIChPcmFjbGUgQ29ycG9yYXRpb24pCgpQSwcIj79DCUoAAABLAAAAUEsDBBQACAgIACVFfFcAAAAAAAAAAAAAAAAMAAAATWVkaWFVdGlscy5jbGFzczWRSwrCQBBER3trbdPxm4BuBHfiBxHFH4hCwJX4ATfFCrAxnWnYgZCTuPIIHkCPYE+lM5NoILPpoqvrVVd1JslCaLB3MpILJ5xRz5gbMeMS+oyeBOc4xSWucYsZN3CHe7zgiQue8YJXvOEdH/jEFz7whW984weZ+Ecm/pGJf2TiH5n4Ryb+kYl/ZOIfmfhHJv6RiX9k4h+Z+Ecm/pGJf2TiH5n4Ryb+kYl/ZOIfGQaaaXzgE1/4xje+8Y1vfOMb3/jGN77xjW98q9c0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdM0TdOI06nO7p48NRQjICAgICAgICAgICAgICAoKCgoKCgoKCgoKCgoKChoqKioqKioqKio;";

const SUCCESS_TTL: u64 = 24 * 60 * 60; // 24 hours in seconds
const FAILURE_TTL: u64 = 10 * 60; // 10 minutes in seconds
const FAILURE_RESET_INTERVAL: u64 = 2 * 60 * 60; // 2 hours in seconds

// ç£ç›˜ç¼“å­˜è·¯å¾„
const CACHE_DIR: &str = ".cache";
const SPIDER_JAR_FILE: &str = "spider.jar";
const SPIDER_META_FILE: &str = "spider.json";

// ================== è¾…åŠ©å‡½æ•° ==================

/// ä»ç£ç›˜åŠ è½½ Spider JAR
fn load_spider_from_disk() -> Option<SpiderInfo> {
    let cache_dir = PathBuf::from(CACHE_DIR);
    let jar_path = cache_dir.join(SPIDER_JAR_FILE);
    let meta_path = cache_dir.join(SPIDER_META_FILE);

    // æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if !jar_path.exists() || !meta_path.exists() {
        return None;
    }

    // è¯»å–å…ƒæ•°æ®
    let meta_content = match std::fs::read_to_string(&meta_path) {
        Ok(content) => content,
        Err(e) => {
            tracing::warn!("Failed to read spider metadata: {}", e);
            return None;
        }
    };

    let metadata: SpiderMetadata = match serde_json::from_str(&meta_content) {
        Ok(meta) => meta,
        Err(e) => {
            tracing::warn!("Failed to parse spider metadata: {}", e);
            return None;
        }
    };

    // æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸï¼ˆ24å°æ—¶ï¼‰
    let now = SystemTime::now()
        .duration_since(SystemTime::UNIX_EPOCH)
        .unwrap()
        .as_secs();

    if now - metadata.timestamp > SUCCESS_TTL {
        tracing::info!("Disk cache expired (age: {}s)", now - metadata.timestamp);
        return None;
    }

    // è¯»å– JAR æ–‡ä»¶
    let buffer = match std::fs::read(&jar_path) {
        Ok(data) => data,
        Err(e) => {
            tracing::warn!("Failed to read spider JAR: {}", e);
            return None;
        }
    };

    // éªŒè¯ MD5
    let actual_md5 = calculate_md5(&buffer);
    if actual_md5 != metadata.md5 {
        tracing::warn!("Spider JAR MD5 mismatch, cache corrupted");
        return None;
    }

    tracing::info!(
        "Loaded spider JAR from disk: {} bytes, md5: {}, age: {}s",
        buffer.len(),
        metadata.md5,
        now - metadata.timestamp
    );

    Some(SpiderInfo {
        buffer: Some(buffer),
        md5: metadata.md5,
        source: metadata.source,
        success: metadata.success,
        cached: true,
        timestamp: SystemTime::UNIX_EPOCH + Duration::from_secs(metadata.timestamp),
        size: metadata.size,
        tried: 0,
    })
}

/// ä¿å­˜ Spider JAR åˆ°ç£ç›˜
fn save_spider_to_disk(info: &SpiderInfo) -> Result<(), String> {
    let cache_dir = PathBuf::from(CACHE_DIR);

    // åˆ›å»ºç¼“å­˜ç›®å½•
    if let Err(e) = std::fs::create_dir_all(&cache_dir) {
        return Err(format!("Failed to create cache directory: {}", e));
    }

    let jar_path = cache_dir.join(SPIDER_JAR_FILE);
    let meta_path = cache_dir.join(SPIDER_META_FILE);

    // ä¿å­˜ JAR æ–‡ä»¶
    if let Some(buffer) = &info.buffer {
        if let Err(e) = std::fs::write(&jar_path, buffer) {
            return Err(format!("Failed to write spider JAR: {}", e));
        }
    } else {
        return Err("No buffer to save".to_string());
    }

    // ä¿å­˜å…ƒæ•°æ®
    let timestamp = info
        .timestamp
        .duration_since(SystemTime::UNIX_EPOCH)
        .unwrap()
        .as_secs();

    let metadata = SpiderMetadata {
        md5: info.md5.clone(),
        source: info.source.clone(),
        success: info.success,
        timestamp,
        size: info.size,
    };

    let meta_content = serde_json::to_string_pretty(&metadata)
        .map_err(|e| format!("Failed to serialize metadata: {}", e))?;

    std::fs::write(&meta_path, meta_content)
        .map_err(|e| format!("Failed to write metadata: {}", e))?;

    tracing::info!(
        "Saved spider JAR to disk: {} bytes, md5: {}",
        info.size,
        info.md5
    );

    Ok(())
}

fn is_private_host(url: &str) -> bool {
    if url.starts_with("http") {
        let lower = url.to_lowercase();
        lower.contains("localhost") || lower.contains("127.") || lower.contains("10.")
    } else {
        true
    }
}

fn get_candidates() -> Vec<String> {
    // ç®€åŒ–ç‰ˆï¼šè¿”å›æ‰€æœ‰å€™é€‰æºï¼ˆå›½å†…ä¼˜å…ˆï¼Œç„¶åå›½é™…ï¼Œæœ€åä»£ç†ï¼‰
    let mut candidates = Vec::new();
    candidates.extend(DOMESTIC_CANDIDATES.iter().map(|s| s.to_string()));
    candidates.extend(INTERNATIONAL_CANDIDATES.iter().map(|s| s.to_string()));
    candidates.extend(PROXY_CANDIDATES.iter().map(|s| s.to_string()));
    candidates
}

fn calculate_md5(data: &[u8]) -> String {
    format!("{:x}", md5::compute(data))
}

async fn fetch_remote(url: &str, timeout_ms: u64, retry_count: usize) -> Option<Vec<u8>> {
    for attempt in 0..=retry_count {
        match fetch_remote_once(url, timeout_ms).await {
            Ok(data) => {
                // éªŒè¯ JAR æ–‡ä»¶æ ¼å¼ï¼ˆæ£€æŸ¥ ZIP å¤´ï¼‰
                if data.len() < 1000 {
                    tracing::warn!("File too small: {} bytes from {}", data.len(), url);
                    continue;
                }

                if data[0] != 0x50 || data[1] != 0x4B {
                    tracing::warn!("Invalid JAR file format from {}", url);
                    continue;
                }

                return Some(data);
            }
            Err(e) => {
                tracing::warn!(
                    "å°è¯• {}/{} å¤±è´¥ï¼Œæº: {}: {}",
                    attempt + 1,
                    retry_count + 1,
                    url,
                    e
                );

                // ç½‘ç»œé”™è¯¯ç­‰å¾…åé‡è¯•
                if attempt < retry_count {
                    tokio::time::sleep(Duration::from_secs((attempt + 1) as u64)).await;
                }
            }
        }
    }

    None
}

async fn fetch_remote_once(url: &str, timeout_ms: u64) -> Result<Vec<u8>, String> {
    let client = reqwest::Client::builder()
        .timeout(Duration::from_millis(timeout_ms))
        .redirect(reqwest::redirect::Policy::limited(5))
        .build()
        .map_err(|e| format!("Failed to create client: {}", e))?;

    // æ ¹æ®æºç±»å‹ä¼˜åŒ–è¯·æ±‚å¤´
    let mut headers = reqwest::header::HeaderMap::new();
    headers.insert("Accept", "*/*".parse().unwrap());
    headers.insert("Accept-Encoding", "identity".parse().unwrap());
    headers.insert("Cache-Control", "no-cache".parse().unwrap());
    headers.insert("Connection", "close".parse().unwrap());

    let user_agent = if url.contains("github") || url.contains("raw.githubusercontent") {
        "curl/7.68.0"
    } else if url.contains("gitee") || url.contains("gitcode") {
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    } else if url.contains("jsdelivr") || url.contains("fastly") {
        "DecoTV/1.0"
    } else {
        "Mozilla/5.0 (Linux; Android 11; SM-G973F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Mobile Safari/537.36"
    };

    headers.insert("User-Agent", user_agent.parse().unwrap());

    let response = client
        .get(url)
        .headers(headers)
        .send()
        .await
        .map_err(|e| format!("Request failed: {}", e))?;

    if !response.status().is_success() {
        return Err(format!(
            "HTTP {}: {}",
            response.status(),
            response.status().canonical_reason().unwrap_or("Unknown")
        ));
    }

    let bytes = response
        .bytes()
        .await
        .map_err(|e| format!("Failed to read response: {}", e))?;

    Ok(bytes.to_vec())
}

async fn get_spider_jar(state: &AppState, force_refresh: bool) -> SpiderInfo {
    let now = SystemTime::now();

    // é‡ç½®å¤±è´¥è®°å½•ï¼ˆå®šæœŸæ¸…ç†ï¼‰
    {
        let mut failed = state.failed_sources.lock().await;
        if let Ok(elapsed) = now.duration_since(failed.last_reset) {
            if elapsed.as_secs() > FAILURE_RESET_INTERVAL {
                failed.sources.clear();
                failed.last_reset = now;
                tracing::info!("é‡ç½®å¤±è´¥çš„æºåˆ—è¡¨");
            }
        }
    }

    // 1. æ£€æŸ¥ç£ç›˜ç¼“å­˜ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
    if !force_refresh {
        if let Some(disk_info) = load_spider_from_disk() {
            // æ›´æ–°å†…å­˜ç¼“å­˜
            let mut cache = state.spider_info.lock().await;
            *cache = disk_info.clone();
            tracing::info!("ä½¿ç”¨ç£ç›˜ç¼“å­˜çš„ spider jar");
            return disk_info;
        }
    }

    // 2. æ£€æŸ¥å†…å­˜ç¼“å­˜
    if !force_refresh {
        let cache = state.spider_info.lock().await;
        // åªæœ‰å½“ç¼“å­˜ä¸­æœ‰å®é™…çš„ JAR æ•°æ®æ—¶æ‰ä½¿ç”¨ç¼“å­˜
        if cache.buffer.is_some() {
            if let Ok(elapsed) = now.duration_since(cache.timestamp) {
                let ttl = if cache.success {
                    SUCCESS_TTL
                } else {
                    FAILURE_TTL
                };
                if elapsed.as_secs() < ttl {
                    tracing::info!(
                        "ä½¿ç”¨ç¼“å­˜çš„ spider jar (age: {}s, success: {})",
                        elapsed.as_secs(),
                        cache.success
                    );
                    return SpiderInfo {
                        cached: true,
                        ..cache.clone()
                    };
                }
            }
        }
    }

    let mut tried = 0;
    let candidates = get_candidates();
    let total_candidates = candidates.len(); // Store length before move

    // è¿‡æ»¤æ‰è¿‘æœŸå¤±è´¥çš„æº
    let failed_sources = state.failed_sources.lock().await;
    let active_candidates: Vec<String> = candidates
        .iter()
        .filter(|url| !failed_sources.sources.contains(*url))
        .cloned()
        .collect();
    drop(failed_sources);

    let candidates_to_try = if active_candidates.is_empty() {
        candidates
    } else {
        active_candidates
    };

    tracing::info!("å°è¯• {} spider jar æº", candidates_to_try.len());

    for url in candidates_to_try {
        tried += 1;
        tracing::info!("å°è¯• spider jar æº {}/{}: {}", tried, total_candidates, url);

        if let Some(buffer) = fetch_remote(&url, 3000, 1).await {
            // æˆåŠŸæ—¶ä»å¤±è´¥åˆ—è¡¨ç§»é™¤
            let mut failed = state.failed_sources.lock().await;
            failed.sources.remove(&url);
            drop(failed);

            let md5_hash = calculate_md5(&buffer);
            let size = buffer.len();

            let info = SpiderInfo {
                buffer: Some(buffer),
                md5: md5_hash,
                source: url.clone(),
                success: true,
                cached: false,
                timestamp: now,
                size,
                tried,
            };

            // æ›´æ–°ç¼“å­˜
            let mut cache = state.spider_info.lock().await;
            *cache = info.clone();
            drop(cache);

            // ä¿å­˜åˆ°ç£ç›˜
            if let Err(e) = save_spider_to_disk(&info) {
                tracing::warn!("ä¿å­˜ spider jar åˆ°ç£ç›˜å¤±è´¥: {}", e);
            }

            tracing::info!(
                "æˆåŠŸä» {} è·å– spider jar (size: {} bytes, md5: {})",
                url,
                size,
                info.md5
            );
            return info;
        } else {
            // å¤±è´¥æ—¶æ·»åŠ åˆ°å¤±è´¥åˆ—è¡¨
            let mut failed = state.failed_sources.lock().await;
            failed.sources.insert(url.clone());
        }
    }

    tracing::warn!("æ‰€æœ‰ spider jar æºå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨");

    let fallback_data = base64::engine::general_purpose::STANDARD
        .decode(FALLBACK_JAR_BASE64)
        .unwrap_or_default();

    let md5_hash = calculate_md5(&fallback_data);
    let size = fallback_data.len();

    let info = SpiderInfo {
        buffer: Some(fallback_data),
        md5: md5_hash,
        source: "fallback".to_string(),
        success: false,
        cached: false,
        timestamp: now,
        size,
        tried,
    };

    // æ›´æ–°ç¼“å­˜
    let mut cache = state.spider_info.lock().await;
    *cache = info.clone();

    info
}

/// ä» URL è·å–è®¢é˜…é…ç½®
async fn fetch_subscription(url: &str) -> Result<SubscriptionConfig, String> {
    tracing::info!("Fetching subscription from: {}", url);

    let client = reqwest::Client::builder()
        .timeout(Duration::from_secs(30))
        .user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
        .build()
        .map_err(|e| format!("Failed to create HTTP client: {}", e))?;

    let response = client
        .get(url)
        .send()
        .await
        .map_err(|e| format!("Failed to fetch subscription: {}", e))?;

    if !response.status().is_success() {
        return Err(format!("HTTP error: {}", response.status()));
    }

    let text = response
        .text()
        .await
        .map_err(|e| format!("Failed to read response: {}", e))?;

    // å°è¯•è§£æè‡ªå®šä¹‰æ ¼å¼ï¼ˆapi_site æ ¼å¼ï¼‰
    if let Ok(custom_format) = serde_json::from_str::<CustomSubscriptionFormat>(&text) {
        tracing::info!(
            "Parsed custom subscription format with {} sites",
            custom_format.api_site.len()
        );
        return Ok(convert_custom_format_to_tvbox(custom_format));
    }

    // å°è¯•è§£ææ ‡å‡† TVBox æ ¼å¼
    serde_json::from_str::<SubscriptionConfig>(&text)
        .map_err(|e| format!("Failed to parse subscription JSON: {}", e))
}

/// å°†è‡ªå®šä¹‰æ ¼å¼è½¬æ¢ä¸º TVBox é…ç½®æ ¼å¼
fn convert_custom_format_to_tvbox(custom: CustomSubscriptionFormat) -> SubscriptionConfig {
    let mut sites = Vec::new();

    for (domain, site_info) in custom.api_site {
        // ä½¿ç”¨åŸŸåä½œä¸º keyï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦
        let key = domain.replace(".", "_").replace("-", "_").replace(":", "_");

        // æ ¹æ® API URL åˆ¤æ–­ç«™ç‚¹ç±»å‹
        // MacCMS API é€šå¸¸åŒ…å« /api.php/provide/vodï¼Œä½¿ç”¨ type: 1
        // å…¶ä»–æƒ…å†µä½¿ç”¨ type: 3 (Spider)
        let site_type = if site_info.api.contains("/api.php/provide/vod")
            || site_info.api.contains("/api.php/provide/")
            || site_info.api.contains("maccms")
        {
            1 // MacCMS èµ„æºç«™
        } else {
            3 // Spider ç«™ç‚¹
        };

        sites.push(Site {
            key,
            name: site_info.name,
            site_type,
            api: site_info.api,
            jar: None,
            is_adult: None,
            searchable: Some(1),
            quick_search: Some(1),
            filterable: Some(1),
            changeable: None,
        });
    }

    SubscriptionConfig {
        spider: Some(
            "https://cdn.jsdelivr.net/gh/FongMi/CatVodSpider@main/jar/spider.jar".to_string(),
        ),
        sites: Some(sites),
        parses: Some(vec![
            Parse {
                name: "é»˜è®¤è§£æ".to_string(),
                parse_type: 0,
                url: "https://jx.xmflv.com/?url=".to_string(),
            },
            Parse {
                name: "å¹¶å‘è§£æ".to_string(),
                parse_type: 2,
                url: "Parallel".to_string(),
            },
        ]),
        lives: None,
    }
}

/// è·å–ç¼“å­˜çš„è®¢é˜…é…ç½®
async fn get_cached_subscription(
    state: &AppState,
    url: &str,
    force_refresh: bool,
) -> Result<SubscriptionConfig, String> {
    let mut cache = state.subscription_cache.lock().await;

    // æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆï¼ˆ10åˆ†é’Ÿï¼‰
    let cache_valid = if let Some(cached) = cache.as_ref() {
        SystemTime::now()
            .duration_since(cached.cached_at)
            .map(|d| d.as_secs() < 600)
            .unwrap_or(false)
    } else {
        false
    };

    if force_refresh || !cache_valid {
        tracing::info!("Fetching fresh subscription data");
        let config = fetch_subscription(url).await?;
        *cache = Some(CachedSubscription {
            config: config.clone(),
            cached_at: SystemTime::now(),
        });
        Ok(config)
    } else {
        Ok(cache.as_ref().unwrap().config.clone())
    }
}

/// é»˜è®¤é…ç½®
fn get_default_config() -> SubscriptionConfig {
    SubscriptionConfig {
        spider: Some(
            "https://cdn.jsdelivr.net/gh/FongMi/CatVodSpider@main/jar/spider.jar".to_string(),
        ),
        sites: Some(vec![Site {
            key: "demo".to_string(),
            name: "æ¼”ç¤ºç«™ç‚¹".to_string(),
            site_type: 3, // Spider ç«™ç‚¹
            api: "https://example.com/api".to_string(),
            jar: None,
            is_adult: Some(false),
            searchable: Some(1),
            quick_search: Some(1),
            filterable: Some(1),
            changeable: None,
        }]),
        parses: Some(vec![
            Parse {
                name: "é»˜è®¤è§£æ".to_string(),
                parse_type: 0,
                url: "https://jx.xmflv.com/?url=".to_string(),
            },
            Parse {
                name: "å¹¶å‘è§£æ".to_string(),
                parse_type: 2,
                url: "Parallel".to_string(),
            },
        ]),
        lives: None,
    }
}

// ================== API å¤„ç†å™¨ ==================

pub async fn get_config_handler(
    Query(params): Query<ConfigParams>,
    State(state): State<AppState>,
) -> impl IntoResponse {
    // 0. æ„å»º API æœåŠ¡å™¨åœ°å€ï¼ˆç”¨äº M3U8 ä»£ç†ï¼‰

    let api_base_url = format!("http://{}", SERVER_IP.to_string());
    let m3u8_proxy_url = format!("{}/api/proxy/m3u8?url=", api_base_url);

    // 1. è·å–è®¢é˜…é…ç½®
    let subscription_url = params
        .subscription_url
        .as_deref()
        .unwrap_or(PARSES_URL.as_str());

    let force_refresh = params.force_spider_refresh.as_deref() == Some("1");

    let mut config = match get_cached_subscription(&state, subscription_url, force_refresh).await {
        Ok(cfg) => cfg,
        Err(e) => {
            tracing::error!("Failed to fetch subscription, using default: {}", e);
            get_default_config()
        }
    };

    // 2. å¤„ç† Spider é€»è¾‘ï¼ˆä½¿ç”¨ 10 ç§’è¶…æ—¶ï¼‰
    let spider_info = match tokio::time::timeout(
        Duration::from_secs(10),
        get_spider_jar(&state, force_refresh),
    )
    .await
    {
        Ok(info) => info,
        Err(_) => {
            tracing::warn!("Spider JAR è·å–è¶…æ—¶ï¼Œä½¿ç”¨å¤‡ç”¨");
            // è¶…æ—¶æ—¶ä½¿ç”¨ fallback
            let fallback_data = base64::engine::general_purpose::STANDARD
                .decode(FALLBACK_JAR_BASE64)
                .unwrap_or_default();
            let md5_hash = calculate_md5(&fallback_data);
            SpiderInfo {
                buffer: Some(fallback_data.clone()),
                md5: md5_hash.clone(),
                source: "fallback".to_string(),
                success: false,
                cached: false,
                timestamp: SystemTime::now(),
                size: fallback_data.len(),
                tried: 0,
            }
        }
    };

    // æ„å»º spider å­—ç¬¦ä¸²ï¼ˆä½¿ç”¨ä»£ç† URLï¼‰
    let spider_proxy_url = format!("http://{}:3000/api/proxy/spider.jar", SERVER_IP.as_str());
    let global_spider_jar = format!("{};md5;{}", spider_proxy_url, spider_info.md5);

    // å…è®¸ URL å‚æ•°è¦†ç›– Spiderï¼ˆä»…å½“æ˜¯å…¬ç½‘åœ°å€æ—¶ï¼‰
    let final_spider = if let Some(spider_url) = &params.spider {
        if spider_url.starts_with("http") && !is_private_host(spider_url) {
            spider_url.clone()
        } else {
            global_spider_jar
        }
    } else {
        // ä¼˜å…ˆä½¿ç”¨ä»£ç† URLï¼Œå¿½ç•¥è®¢é˜…é…ç½®ä¸­çš„ spider
        global_spider_jar
    };

    // 3. è¿‡æ»¤é€»è¾‘
    let filter_adult = !matches!(params.filter.as_deref(), Some("off") | Some("disable"))
        && !matches!(params.adult.as_deref(), Some("1") | Some("true"));

    // 4. åº”ç”¨è¿‡æ»¤
    if filter_adult {
        if let Some(sites) = config.sites.as_mut() {
            sites.retain(|s| !s.is_adult.unwrap_or(false));
        }
    }

    let mode = params
        .mode
        .clone()
        .unwrap_or_else(|| "standard".to_string());

    // 5. æ·»åŠ å¹¿å‘Šè¿‡æ»¤è§£æå™¨
    let mut parses = config.parses.unwrap_or_default();

    // åœ¨å¼€å¤´æ’å…¥å¹¿å‘Šè¿‡æ»¤è§£æå™¨ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰
    parses.insert(
        0,
        Parse {
            name: "ğŸš« å¹¿å‘Šè¿‡æ»¤".to_string(),
            parse_type: 0,
            url: m3u8_proxy_url,
        },
    );

    // 6. ç»„è£…å“åº”ï¼ˆä»…è¿”å› TVBox æ ‡å‡†å­—æ®µï¼‰
    let response = serde_json::json!({
        "spider": final_spider,
        "sites": config.sites.unwrap_or_default(),
        "parses": parses,
        "lives": config.lives.unwrap_or_default(),
    });

    tracing::info!(
        "Config generated: spider_success={}, mode={}, filter_adult={}, subscription_url={}",
        spider_info.success,
        mode,
        filter_adult,
        subscription_url
    );

    (StatusCode::OK, Json(response))
}

/// M3U8 ä»£ç†å¤„ç†å™¨ï¼ˆå¸¦å¹¿å‘Šè¿‡æ»¤ï¼‰
pub async fn proxy_m3u8_handler(
    Query(params): Query<std::collections::HashMap<String, String>>,
) -> impl IntoResponse {
    let url = match params.get("url") {
        Some(u) => u.clone(), // å…‹éš†ä»¥é¿å…ç”Ÿå‘½å‘¨æœŸé—®é¢˜
        None => return (StatusCode::BAD_REQUEST, "Missing url parameter".to_string()),
    };

    tracing::info!("Proxying M3U8: {}", url);

    let client = match reqwest::Client::builder()
        .timeout(Duration::from_secs(30))
        .user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
        .build()
    {
        Ok(c) => c,
        Err(e) => {
            return (
                StatusCode::INTERNAL_SERVER_ERROR,
                format!("Failed to create client: {}", e),
            )
        }
    };

    let response = match client.get(&url).send().await {
        Ok(r) => r,
        Err(e) => {
            return (
                StatusCode::BAD_GATEWAY,
                format!("Failed to fetch M3U8: {}", e),
            )
        }
    };

    if !response.status().is_success() {
        return (
            StatusCode::BAD_GATEWAY,
            format!("Upstream error: {}", response.status()),
        );
    }

    let content = match response.text().await {
        Ok(c) => c,
        Err(e) => {
            return (
                StatusCode::INTERNAL_SERVER_ERROR,
                format!("Failed to read response: {}", e),
            )
        }
    };

    // ä½¿ç”¨ core crate ä¸­çš„å¹¿å‘Šè¿‡æ»¤å‡½æ•°
    let filtered = filter_ads_from_m3_u8(&content);

    // é‡å†™ M3U8 ä¸­çš„ TS URL ä¸ºä»£ç† URL
    let proxy_base = format!("http://{}:3000/api/proxy/ts?url=", SERVER_IP.as_str());
    let rewritten = rewrite_m3u8_urls(&filtered, &url, &proxy_base);

    // åå°å¹¶å‘é¢„åŠ è½½å‰å‡ ä¸ª TS ç‰‡æ®µï¼ˆå¼‚æ­¥ï¼Œä¸é˜»å¡å“åº”ï¼‰
    let rewritten_clone = rewritten.clone();
    let url_clone = url.clone();
    tokio::spawn(async move {
        preload_ts_segments(&rewritten_clone, &url_clone).await;
    });

    (StatusCode::OK, rewritten)
}

/// é‡å†™ M3U8 ä¸­çš„ TS URL ä¸ºä»£ç† URL
fn rewrite_m3u8_urls(m3u8_content: &str, base_url: &str, proxy_base: &str) -> String {
    let mut result = String::new();

    for line in m3u8_content.lines() {
        let trimmed = line.trim();

        // è·³è¿‡æ³¨é‡Šè¡Œå’Œç©ºè¡Œ
        if trimmed.starts_with('#') || trimmed.is_empty() {
            result.push_str(line);
            result.push('\n');
            continue;
        }

        // å¤„ç† TS URL
        if trimmed.ends_with(".ts") || trimmed.ends_with(".m3u8") {
            // è§£æä¸ºç»å¯¹ URL
            let absolute_url = if trimmed.starts_with("http://") || trimmed.starts_with("https://")
            {
                trimmed.to_string()
            } else {
                // ç›¸å¯¹ URLï¼Œéœ€è¦åŸºäº base_url è§£æ
                resolve_relative_url(base_url, trimmed)
            };

            // é‡å†™ä¸ºä»£ç† URL
            let encoded_url = urlencoding::encode(&absolute_url);
            result.push_str(&format!("{}{}", proxy_base, encoded_url));
            result.push('\n');
        } else {
            result.push_str(line);
            result.push('\n');
        }
    }

    result
}

/// è§£æç›¸å¯¹ URL
fn resolve_relative_url(base: &str, relative: &str) -> String {
    if let Ok(base_url) = url::Url::parse(base) {
        if let Ok(resolved) = base_url.join(relative) {
            return resolved.to_string();
        }
    }
    relative.to_string()
}

/// å¹¶å‘é¢„åŠ è½½ TS ç‰‡æ®µï¼ˆå‰5ä¸ªï¼‰
async fn preload_ts_segments(m3u8_content: &str, _base_url: &str) {
    let mut ts_urls = Vec::new();

    for line in m3u8_content.lines() {
        let trimmed = line.trim();
        if trimmed.ends_with(".ts") {
            // æå–åŸå§‹ URLï¼ˆä»ä»£ç† URL ä¸­è§£ç ï¼‰
            if let Some(url_param) = trimmed
                .strip_prefix("http://")
                .and_then(|s| s.split("url=").nth(1))
            {
                if let Ok(decoded) = urlencoding::decode(url_param) {
                    ts_urls.push(decoded.to_string());
                    if ts_urls.len() >= 5 {
                        break;
                    }
                }
            }
        }
    }

    // å¹¶å‘ä¸‹è½½å‰5ä¸ªç‰‡æ®µ
    let tasks: Vec<_> = ts_urls
        .into_iter()
        .map(|url| {
            tokio::spawn(async move {
                if let Ok(client) = reqwest::Client::builder()
                    .timeout(Duration::from_secs(10))
                    .build()
                {
                    let _ = client.get(&url).send().await;
                }
            })
        })
        .collect();

    // ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
    for task in tasks {
        let _ = task.await;
    }
}

/// TS è§†é¢‘ç‰‡æ®µä»£ç†å¤„ç†å™¨ï¼ˆå¸¦ç¼“å­˜åŠ é€Ÿï¼‰
pub async fn proxy_ts_handler(
    Query(params): Query<std::collections::HashMap<String, String>>,
    State(state): State<AppState>,
) -> impl IntoResponse {
    let url = match params.get("url") {
        Some(u) => u,
        None => {
            return (
                StatusCode::BAD_REQUEST,
                axum::http::HeaderMap::new(),
                "Missing url parameter".as_bytes().to_vec(),
            )
        }
    };

    // æ£€æŸ¥ç¼“å­˜
    if let Some(cached_data) = state.ts_cache.get(url).await {
        tracing::debug!("TS ç¼“å­˜: {}", url);
        let mut headers = axum::http::HeaderMap::new();
        headers.insert(
            axum::http::header::CONTENT_TYPE,
            "video/mp2t".parse().unwrap(),
        );
        headers.insert(
            axum::http::header::CACHE_CONTROL,
            "public, max-age=3600".parse().unwrap(),
        );
        return (StatusCode::OK, headers, cached_data);
    }

    tracing::debug!("TS cache miss, downloading: {}", url);

    // ä¸‹è½½ TS ç‰‡æ®µ
    let client = match reqwest::Client::builder()
        .timeout(Duration::from_secs(30))
        .user_agent("Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36")
        .build()
    {
        Ok(c) => c,
        Err(e) => {
            return (
                StatusCode::INTERNAL_SERVER_ERROR,
                axum::http::HeaderMap::new(),
                format!("Failed to create client: {}", e).into_bytes(),
            )
        }
    };

    let response = match client.get(url).send().await {
        Ok(r) => r,
        Err(e) => {
            return (
                StatusCode::BAD_GATEWAY,
                axum::http::HeaderMap::new(),
                format!("Failed to fetch TS: {}", e).into_bytes(),
            )
        }
    };

    if !response.status().is_success() {
        return (
            StatusCode::BAD_GATEWAY,
            axum::http::HeaderMap::new(),
            format!("Upstream error: {}", response.status()).into_bytes(),
        );
    }

    let data = match response.bytes().await {
        Ok(d) => d.to_vec(),
        Err(e) => {
            return (
                StatusCode::INTERNAL_SERVER_ERROR,
                axum::http::HeaderMap::new(),
                format!("Failed to read response: {}", e).into_bytes(),
            )
        }
    };

    // ä¿å­˜åˆ°ç¼“å­˜
    state.ts_cache.insert(url.clone(), data.clone()).await;
    tracing::debug!("TS cached: {} ({} bytes)", url, data.len());

    let mut headers = axum::http::HeaderMap::new();
    headers.insert(
        axum::http::header::CONTENT_TYPE,
        "video/mp2t".parse().unwrap(),
    );
    headers.insert(
        axum::http::header::CACHE_CONTROL,
        "public, max-age=3600".parse().unwrap(),
    );

    (StatusCode::OK, headers, data)
}

/// Spider JAR ä»£ç†å¤„ç†å™¨
pub async fn proxy_spider_jar_handler(State(state): State<AppState>) -> impl IntoResponse {
    tracing::info!("Proxying Spider JAR");

    // è·å–ç¼“å­˜çš„ Spider JARï¼ˆä¸å¼ºåˆ¶åˆ·æ–°ï¼‰
    let spider_info = get_spider_jar(&state, false).await;

    if let Some(buffer) = spider_info.buffer {
        tracing::info!(
            "Serving Spider JAR: {} bytes, md5: {}, source: {}",
            buffer.len(),
            spider_info.md5,
            spider_info.source
        );

        // è®¾ç½®å“åº”å¤´
        let mut headers = axum::http::HeaderMap::new();
        headers.insert(
            axum::http::header::CONTENT_TYPE,
            "application/java-archive".parse().unwrap(),
        );
        headers.insert(
            axum::http::header::CONTENT_DISPOSITION,
            "attachment; filename=\"spider.jar\"".parse().unwrap(),
        );
        headers.insert(
            axum::http::header::CACHE_CONTROL,
            "public, max-age=14400".parse().unwrap(), // 4 hours
        );

        (StatusCode::OK, headers, buffer)
    } else {
        tracing::error!("Failed to get Spider JAR");
        (
            StatusCode::INTERNAL_SERVER_ERROR,
            axum::http::HeaderMap::new(),
            Vec::new(),
        )
    }
}
